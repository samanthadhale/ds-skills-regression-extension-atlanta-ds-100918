{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check out the following tutorial from sklearn:\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#sphx-glr-auto-examples-exercises-plot-cv-diabetes-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer to the bonus question: how much can you trust the selection of alpha?\n",
      "\n",
      "Alpha parameters maximising the generalization score on different\n",
      "subsets of the data:\n",
      "[fold 0] alpha: 0.05968, score: 0.54209\n",
      "[fold 1] alpha: 0.04520, score: 0.15523\n",
      "[fold 2] alpha: 0.07880, score: 0.45193\n",
      "\n",
      "Answer: Not very much since we obtained different alphas for different\n",
      "subsets of the data and moreover, the scores for these alphas differ\n",
      "quite substantially.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data[:150]\n",
    "y = diabetes.target[:150]\n",
    "\n",
    "lasso = Lasso(random_state=0)\n",
    "alphas = np.logspace(-4, -0.5, 30)\n",
    "\n",
    "tuned_parameters = [{'alpha': alphas}]\n",
    "n_folds = 5\n",
    "\n",
    "clf = GridSearchCV(lasso, tuned_parameters, cv=n_folds, refit=False)\n",
    "clf.fit(X, y)\n",
    "scores = clf.cv_results_['mean_test_score']\n",
    "scores_std = clf.cv_results_['std_test_score']\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alphas, scores)\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "std_error = scores_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.semilogx(alphas, scores + std_error, 'b--')\n",
    "plt.semilogx(alphas, scores - std_error, 'b--')\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])\n",
    "\n",
    "# #############################################################################\n",
    "# Bonus: how much can you trust the selection of alpha?\n",
    "\n",
    "# To answer this question we use the LassoCV object that sets its alpha\n",
    "# parameter automatically from the data by internal cross-validation (i.e. it\n",
    "# performs cross-validation on the training data it receives).\n",
    "# We use external cross-validation to see how much the automatically obtained\n",
    "# alphas differ across different cross-validation folds.\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=0)\n",
    "k_fold = KFold(3)\n",
    "\n",
    "print(\"Answer to the bonus question:\",\n",
    "      \"how much can you trust the selection of alpha?\")\n",
    "print()\n",
    "print(\"Alpha parameters maximising the generalization score on different\")\n",
    "print(\"subsets of the data:\")\n",
    "for k, (train, test) in enumerate(k_fold.split(X, y)):\n",
    "    lasso_cv.fit(X[train], y[train])\n",
    "    print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n",
    "          format(k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])))\n",
    "print()\n",
    "print(\"Answer: Not very much since we obtained different alphas for different\")\n",
    "print(\"subsets of the data and moreover, the scores for these alphas differ\")\n",
    "print(\"quite substantially.\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss your comments on the confidence interval bands shown on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
